{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to My Lab For more information about me and other projects visit christianlowe.com . Project layout .github/ # Github configuration, workflows, etc ansible/ # Ansible Playbooks bruno/ # Bruno API Requests docker/ # Docker compose files docs/ # Docs drf/ # Django REST Framework Playground expo/ # Expo Playground infra/ # Terraform and other IaC k8s/ # k8s Playground - Service Manifests and Cluster Config krakend/ # KrakenD Config & Plugins scripts/ # Misc Scripts mkdocs.yml # Documentation configuration Documentation Build the docker image in docker/mkdocs docker build . -t mkdocs Run the following command to start a server with live reload docker run --rm -v \"`pwd`:/app\" -w /app -p 8000:8000 mkdocs mkdocs serve -a 0.0.0.0:8000 The docs will be served at http://localhost:8000","title":"Home"},{"location":"#welcome-to-my-lab","text":"For more information about me and other projects visit christianlowe.com .","title":"Welcome to My Lab"},{"location":"#project-layout","text":".github/ # Github configuration, workflows, etc ansible/ # Ansible Playbooks bruno/ # Bruno API Requests docker/ # Docker compose files docs/ # Docs drf/ # Django REST Framework Playground expo/ # Expo Playground infra/ # Terraform and other IaC k8s/ # k8s Playground - Service Manifests and Cluster Config krakend/ # KrakenD Config & Plugins scripts/ # Misc Scripts mkdocs.yml # Documentation configuration","title":"Project layout"},{"location":"#documentation","text":"Build the docker image in docker/mkdocs docker build . -t mkdocs Run the following command to start a server with live reload docker run --rm -v \"`pwd`:/app\" -w /app -p 8000:8000 mkdocs mkdocs serve -a 0.0.0.0:8000 The docs will be served at http://localhost:8000","title":"Documentation"},{"location":"ansible/","text":"Ansible Playbooks Be sure to have your ansible config created. See these docs to get started creating your config. Creating an ansible config is outside the scope of this documentation and is better left to the official ansible documentation. When you define your hosts, be sure to include a group named clowe-app-webservers . These are the hosts that will get the docker containers created. single-debian.yml Use this playbook to deploy the service to a single server whether it's an EC2 Instance, Digital Ocean Droplet, GCP Compute Engine, Azure Virtual Machine, etc. Great for testing or small deployments. For instance if you're just running it for yourself or a handful of people. From the root directory run ansible-playbook ansible/single-debian.yml If you want to use a cloudflare tunnel to access the docker container externally ansible-playbook -e run_cloudflare_container=true -e tunnel_token=${TOKEN} ansible/single-debian.yml Setting up a user with the correct permissions to become root on the remote host is currently outside the scope of this tutorial. maintenance/db-backup.yml Copy and rename the ansible/maintenance/s3-backup-vars-sample.json file to s3-backup-vars.json in the ansible/maintenace folder. Replace the values from values obtained from applying the terraform. Run the command to take a backup of the database for the django rest framework app running in docker compose ansible-playbook -e @ansible/maintenance/s3-backups-vars.json ansible/maintenance/db-backup.yml -vvvv","title":"Ansible Playbooks"},{"location":"ansible/#ansible-playbooks","text":"Be sure to have your ansible config created. See these docs to get started creating your config. Creating an ansible config is outside the scope of this documentation and is better left to the official ansible documentation. When you define your hosts, be sure to include a group named clowe-app-webservers . These are the hosts that will get the docker containers created.","title":"Ansible Playbooks"},{"location":"ansible/#single-debianyml","text":"Use this playbook to deploy the service to a single server whether it's an EC2 Instance, Digital Ocean Droplet, GCP Compute Engine, Azure Virtual Machine, etc. Great for testing or small deployments. For instance if you're just running it for yourself or a handful of people. From the root directory run ansible-playbook ansible/single-debian.yml If you want to use a cloudflare tunnel to access the docker container externally ansible-playbook -e run_cloudflare_container=true -e tunnel_token=${TOKEN} ansible/single-debian.yml Setting up a user with the correct permissions to become root on the remote host is currently outside the scope of this tutorial.","title":"single-debian.yml"},{"location":"ansible/#maintenancedb-backupyml","text":"Copy and rename the ansible/maintenance/s3-backup-vars-sample.json file to s3-backup-vars.json in the ansible/maintenace folder. Replace the values from values obtained from applying the terraform. Run the command to take a backup of the database for the django rest framework app running in docker compose ansible-playbook -e @ansible/maintenance/s3-backups-vars.json ansible/maintenance/db-backup.yml -vvvv","title":"maintenance/db-backup.yml"},{"location":"commands/","text":"Backup Docker Volume docker run --rm \\ -v \"clowe-app-db-data\":/backup-volume \\ -v \"$(pwd)\":/backup \\ busybox \\ tar -zcvf /backup/my-backup.tar.gz /backup-volume Unzip Backed Up Docker Volume docker run --rm -v \"$(pwd)\":/unzipped busybox tar -xzvf /unzipped/my-backup.tar.gz -C /unzipped Monitor packets on port with tcpdump (CLI) tcpdump -n -v -i any port 53 Install tcpdump on alpine apk update apk fetch tcpdump apk add tcpdump*","title":"Index"},{"location":"commands/#backup-docker-volume","text":"docker run --rm \\ -v \"clowe-app-db-data\":/backup-volume \\ -v \"$(pwd)\":/backup \\ busybox \\ tar -zcvf /backup/my-backup.tar.gz /backup-volume","title":"Backup Docker Volume"},{"location":"commands/#unzip-backed-up-docker-volume","text":"docker run --rm -v \"$(pwd)\":/unzipped busybox tar -xzvf /unzipped/my-backup.tar.gz -C /unzipped Monitor packets on port with tcpdump (CLI) tcpdump -n -v -i any port 53 Install tcpdump on alpine apk update apk fetch tcpdump apk add tcpdump*","title":"Unzip Backed Up Docker Volume"},{"location":"drf/","text":"Backend Development Backend If you don't already have poetry installed, install it using the instructions found here . If you want to watch me struggle getting poetry upgraded from 1.8 to 2.0 click here cd drf Run poetry install Run poetry shell - If you are using poetry version 2.X+ be sure to have the poetry plugin shell installed. python manage.py runserver 0.0.0.0:8000 API Server should be running at http://localhost:8000 in your browser","title":"DRF"},{"location":"drf/#backend-development","text":"","title":"Backend Development"},{"location":"drf/#backend","text":"If you don't already have poetry installed, install it using the instructions found here . If you want to watch me struggle getting poetry upgraded from 1.8 to 2.0 click here cd drf Run poetry install Run poetry shell - If you are using poetry version 2.X+ be sure to have the poetry plugin shell installed. python manage.py runserver 0.0.0.0:8000 API Server should be running at http://localhost:8000 in your browser","title":"Backend"},{"location":"expo/","text":"Frontend Development The frontend is made with Expo brew upgrade nodenv nodenv install Running Locally after Building Native Binary cd frontend npm run ios npm run web npm run android Running with Expo Go npx expo start","title":"Expo"},{"location":"expo/#frontend-development","text":"The frontend is made with Expo brew upgrade nodenv nodenv install","title":"Frontend Development"},{"location":"expo/#running-locally-after-building-native-binary","text":"cd frontend npm run ios npm run web npm run android","title":"Running Locally after Building Native Binary"},{"location":"expo/#running-with-expo-go","text":"npx expo start","title":"Running with Expo Go"},{"location":"expo/building-locally/","text":"building locally To build the application locally the eas-cli tool is required and an eas.json file is needed Command to run for a development build npx eas build --platform ios -e development --local Command to run for a production build npx eas build --platform ios --local --output app.ipa --message \"build\" Submitting App to the App Store npx eas submit -p ios --path app.ipa Doc Links Creating Your First Build with Expo Building App for App Stores Submitting the App to the App Stores Upload Builds to App Store from CLI","title":"building locally"},{"location":"expo/building-locally/#building-locally","text":"To build the application locally the eas-cli tool is required and an eas.json file is needed Command to run for a development build npx eas build --platform ios -e development --local Command to run for a production build npx eas build --platform ios --local --output app.ipa --message \"build\" Submitting App to the App Store npx eas submit -p ios --path app.ipa","title":"building locally"},{"location":"expo/building-locally/#doc-links","text":"Creating Your First Build with Expo Building App for App Stores Submitting the App to the App Stores Upload Builds to App Store from CLI","title":"Doc Links"},{"location":"infra/","text":"Infrastructure Infra is controlled using OpenTofu (Terraform). Cloudflare docs https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs Authenticate by setting the CLOUDFLARE_API_TOKEN environment variable Need a Cloudflare Token with the following permissions","title":"Infrastructure"},{"location":"infra/#infrastructure","text":"Infra is controlled using OpenTofu (Terraform). Cloudflare docs https://registry.terraform.io/providers/cloudflare/cloudflare/latest/docs Authenticate by setting the CLOUDFLARE_API_TOKEN environment variable Need a Cloudflare Token with the following permissions","title":"Infrastructure"},{"location":"k8s/","text":"Kubernetes Local I use kind to run kubernetes locally. And because I'm on Mac I need this nifty tool... that will be explained later. # Install via Homebrew $ brew install chipmk/tap/docker-mac-net-connect # Run the service and register it to launch at boot $ sudo brew services start chipmk/tap/docker-mac-net-connect Install the k8s dashboard with helm helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --namespace kubernetes-dashboard This performs some sort of magic that allows me to define things like this in the k8s/dashboard-lb-service.yml and when I apply the helm chart with helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --namespace kubernetes-dashboard -f dashboard-values.yml when I run kubectl get services -n kubernetes-dashboard and see NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard-api ClusterIP 10.96.230.175 <none> 8000/TCP 6h kubernetes-dashboard-auth ClusterIP 10.96.68.80 <none> 8000/TCP 6h kubernetes-dashboard-kong-proxy LoadBalancer 10.96.147.213 172.29.0.9 443:32040/TCP 6h kubernetes-dashboard-metrics-scraper ClusterIP 10.96.56.194 <none> 8000/TCP 6h kubernetes-dashboard-web ClusterIP 10.96.216.86 <none> 8000/TCP 6h I know I can access my kubernetes dashboard in my browser by visiting http://172.29.0.9 Cilium installed with helm. https://docs.cilium.io/en/stable/installation/k8s-install-helm/ Links - kind cilium load balancer See all spec variables kubectl explain CiliumL2AnnouncementPolicy.spec --recursive See all resource types kubectl api-resources","title":"Kubernetes"},{"location":"k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"k8s/#local","text":"I use kind to run kubernetes locally. And because I'm on Mac I need this nifty tool... that will be explained later. # Install via Homebrew $ brew install chipmk/tap/docker-mac-net-connect # Run the service and register it to launch at boot $ sudo brew services start chipmk/tap/docker-mac-net-connect Install the k8s dashboard with helm helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --namespace kubernetes-dashboard This performs some sort of magic that allows me to define things like this in the k8s/dashboard-lb-service.yml and when I apply the helm chart with helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --namespace kubernetes-dashboard -f dashboard-values.yml when I run kubectl get services -n kubernetes-dashboard and see NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard-api ClusterIP 10.96.230.175 <none> 8000/TCP 6h kubernetes-dashboard-auth ClusterIP 10.96.68.80 <none> 8000/TCP 6h kubernetes-dashboard-kong-proxy LoadBalancer 10.96.147.213 172.29.0.9 443:32040/TCP 6h kubernetes-dashboard-metrics-scraper ClusterIP 10.96.56.194 <none> 8000/TCP 6h kubernetes-dashboard-web ClusterIP 10.96.216.86 <none> 8000/TCP 6h I know I can access my kubernetes dashboard in my browser by visiting http://172.29.0.9 Cilium installed with helm. https://docs.cilium.io/en/stable/installation/k8s-install-helm/","title":"Local"},{"location":"k8s/#links","text":"","title":"Links"},{"location":"k8s/#-kind-cilium-load-balancer","text":"See all spec variables kubectl explain CiliumL2AnnouncementPolicy.spec --recursive See all resource types kubectl api-resources","title":"- kind cilium load balancer"},{"location":"k8s/cilium/","text":"Debugging","title":"Index"},{"location":"k8s/cilium/#debugging","text":"","title":"Debugging"},{"location":"k8s/dashboard/","text":"Install helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --create-namespace --namespace kubernetes-dashboard --values dashboard/values.yml Create external load balancer to access dashboard kubectl apply -f dashboard/dashboard-user.yml kubectl apply -f dashboard/dashboard-user-rbac.yml kubectl -n kubernetes-dashboard create token admin-user Can connect with kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 Access in browser at https://localhost:8443 Or if the dashboard was created with the values.yml file you can access it in the browser using the external ip of the load balancer","title":"Index"},{"location":"k8s/dashboard/#install","text":"helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \\ --create-namespace --namespace kubernetes-dashboard --values dashboard/values.yml Create external load balancer to access dashboard kubectl apply -f dashboard/dashboard-user.yml kubectl apply -f dashboard/dashboard-user-rbac.yml kubectl -n kubernetes-dashboard create token admin-user Can connect with kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 Access in browser at https://localhost:8443 Or if the dashboard was created with the values.yml file you can access it in the browser using the external ip of the load balancer","title":"Install"},{"location":"k8s/pihole/","text":"Debugging Drop into the pihole dns container and perform a dns query to confirm it's working kubectl exec -it $(kubectl get pods -l app=pihole -o name -n pihole-dns) -n pihole-dns -- /bin/bash nslookup google.com 127.0.0.1 Running a debug netcat/nslookup/nmap container kubectl run -it --rm --restart=Never debug --image=nicolaka/netshoot -n pihole-dns -- bash Get Internal/External Ip for pihole kubectl get services -n pihole-dns Query DNS using the pihole servier nslookup google.com {ip_from_above} Get logs for pihole pod kubectl logs -n pihole-dns $(kubectl get pods -n pihole-dns -l app=pihole -o name)","title":"Index"},{"location":"k8s/pihole/#debugging","text":"Drop into the pihole dns container and perform a dns query to confirm it's working kubectl exec -it $(kubectl get pods -l app=pihole -o name -n pihole-dns) -n pihole-dns -- /bin/bash nslookup google.com 127.0.0.1 Running a debug netcat/nslookup/nmap container kubectl run -it --rm --restart=Never debug --image=nicolaka/netshoot -n pihole-dns -- bash Get Internal/External Ip for pihole kubectl get services -n pihole-dns Query DNS using the pihole servier nslookup google.com {ip_from_above} Get logs for pihole pod kubectl logs -n pihole-dns $(kubectl get pods -n pihole-dns -l app=pihole -o name)","title":"Debugging"},{"location":"kind/","text":"KinD Cluster Run a local High Availability (HA) cluster using the following command from the project root kind create cluster --config kind/cluster.yml The cluster is configured without the default CNI or Kube Proxy as Cilium is used to replace the functionality of both. Cilium can be installed using helm and the values found in k8s/cilium/cilium-kind-values.yml helm install cilium cilium/cilium --version 1.17.2 \\ --namespace kube-system --values k8s/cilium/cilium-kind-values.yml For information regarding all of the possible values for the Cilium helm chart, refer to the official docs here . Monitor the cilium deployment status cilium status --wait Deploy the cilium load balancer config kubectl apply -f k8s/cilium/cilium-lb.yml This will allow cilium to provision bare metal load balancers to route to Kubernetes deployments. Deploy the pihole service kubectl apply -f k8s/pihole/manifest.yml Deploy the external dns service kubectl apply -f k8s/external-dns/manifest.yml Deploy the keycloak service kubectl apply -f k8s/keycloak/manifest.yml Deploy Dashy","title":"KinD Cluster"},{"location":"kind/#kind-cluster","text":"Run a local High Availability (HA) cluster using the following command from the project root kind create cluster --config kind/cluster.yml The cluster is configured without the default CNI or Kube Proxy as Cilium is used to replace the functionality of both. Cilium can be installed using helm and the values found in k8s/cilium/cilium-kind-values.yml helm install cilium cilium/cilium --version 1.17.2 \\ --namespace kube-system --values k8s/cilium/cilium-kind-values.yml For information regarding all of the possible values for the Cilium helm chart, refer to the official docs here . Monitor the cilium deployment status cilium status --wait Deploy the cilium load balancer config kubectl apply -f k8s/cilium/cilium-lb.yml This will allow cilium to provision bare metal load balancers to route to Kubernetes deployments. Deploy the pihole service kubectl apply -f k8s/pihole/manifest.yml Deploy the external dns service kubectl apply -f k8s/external-dns/manifest.yml Deploy the keycloak service kubectl apply -f k8s/keycloak/manifest.yml Deploy Dashy","title":"KinD Cluster"},{"location":"links/","text":"Links Validating Apple JWT with Python Expo Apple Authentication Auth with Expo Router Apple Identity Token Info Github API as Github App Combining Sessions and JWT UDM, BPG, K8s, Cilium Link 1 Link 2 Link 3 Link 4 Link 5","title":"Links"},{"location":"links/#links","text":"Validating Apple JWT with Python Expo Apple Authentication Auth with Expo Router Apple Identity Token Info Github API as Github App Combining Sessions and JWT","title":"Links"},{"location":"links/#udm-bpg-k8s-cilium","text":"Link 1 Link 2 Link 3 Link 4 Link 5","title":"UDM, BPG, K8s, Cilium"},{"location":"network/","text":"Network Setup architecture-beta group internet(internet)[Internet] group homelab(cloud)[HomeLab] group media(cloud)[Media] in homelab group services(cloud)[Services] in homelab group vpn(cloud)[VPN] in homelab service user(mdi:user)[User] in internet service smartTv(mdi:tv)[SmartTV] in media service udmpro(internet)[UDM Pro] in homelab service truenas(disk)[TrueNAS] in services service thing(mdi:ab-testing)[blah] in services user:R --> L:udmpro smartTv{group}:R --> L:truenas","title":"Network"},{"location":"network/#network-setup","text":"architecture-beta group internet(internet)[Internet] group homelab(cloud)[HomeLab] group media(cloud)[Media] in homelab group services(cloud)[Services] in homelab group vpn(cloud)[VPN] in homelab service user(mdi:user)[User] in internet service smartTv(mdi:tv)[SmartTV] in media service udmpro(internet)[UDM Pro] in homelab service truenas(disk)[TrueNAS] in services service thing(mdi:ab-testing)[blah] in services user:R --> L:udmpro smartTv{group}:R --> L:truenas","title":"Network Setup"},{"location":"observability/","text":"Observability Options Opensearch Grafana Prometheus https://openobserve.ai/","title":"Observability Options"},{"location":"observability/#observability-options","text":"Opensearch Grafana Prometheus https://openobserve.ai/","title":"Observability Options"}]}